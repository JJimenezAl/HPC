
#
SlurmctldHost=ServMASTER
#SlurmctldHost=
#
DisableRootJobs=NO
EnforcePartLimits=NO
#FirstJobId=1
#MaxJobId=999999
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=all_partitions
#LaunchParameters=send_gids
KillOnBadExit=1
#LaunchType=launch/slurm
#Licenses=foo*4,bar
MailProg=/bin/mail
#MaxJobCount=5000
#MaxStepCount=40000
#MaxTasksPerNode=128
#MpiDefault=none
#MpiParams=ports=12000-12099
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/cgroup
PropagatePrioProcess=0
PropagateResourceLimits=NONE
#PropagateResourceLimitsExcept=MEMLOCK,CPU,NOFILE
#RebootProgram=
ReturnToService=1
SallocDefaultCommand="srun -n1 -N1 --mem-per-cpu=0 --pty --mpi=none $SHELL"
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/d
SlurmUser=slurm
SlurmdUser=root
StateSaveLocation=/var/spool/slurm/ctld
SwitchType=switch/none
# Extend TaskPlugin with task/affinity to if OS is affected by hwloc bug < 1.10
#TaskPlugin=task/cgroup,task/affinity
TaskPlugin=task/cgroup
TaskPluginParam=Sched
MemLimitEnforce=NO
#TopologyPlugin=topology/tree
TmpFS=/data/localscratch
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
#
# PROLOG & EPILOG
# Uncomment only if pam_slurm_adopt is enabled
#PrologFlags=Alloc,Contain
#EpilogSlurmctld=
#PrologSlurmctld=
Prolog=/etc/slurm/prolog/job.sh
Epilog=/etc/slurm/epilog/job.sh
SrunEpilog=/etc/slurm/epilog/srun.sh
SrunProlog=/etc/slurm/prolog/srun.sh
TaskEpilog=/etc/slurm/epilog/task.sh
TaskProlog=/etc/slurm/prolog/task.sh
#
#
# TIMERS
BatchStartTimeout=300
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=300
#HealthCheckProgram=/usr/sbin/nhc
InactiveLimit=120
KillWait=30
MessageTimeout=20
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
DefMemPerCPU=1024
FastSchedule=1
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
#
# HPCNow! suggestion:
# Update the bf_window to the longest partition/QoS limit.
SchedulerParameters=bf_busy_nodes,bf_max_job_start=200,bf_window=1440,bf_continue,default_queue_depth=500,kill_invalid_depend
SelectType=select/cons_res
#
# HPCNow! suggestion:
# CR_CPU_Memory let you schedule on a per-thread basis and can cause resource
# contention between different jobs running on the same core. CR_Core_Memory will
# avoid those issues.
SelectTypeParameters=CR_Core_Memory
#
#
# JOB PRIORITY
# With MAX_TRES, the billable TRES is calculated as the MAX of individual TRES'
# on a node (e.g. cpus, mem, gres) plus the sum of all global TRES' (e.g. licenses)
#PriorityFlags=MAX_TRES,DEPTH_OBLIVIOUS,SMALL_RELATIVE_TO_TIME
PriorityFlags=FAIR_TREE
PriorityType=priority/multifactor
PriorityDecayHalfLife=0
PriorityCalcPeriod=5
PriorityFavorSmall=NO
PriorityMaxAge=10-0
PriorityUsageResetPeriod=NONE
PriorityWeightAge=100000
PriorityWeightFairshare=100000
PriorityWeightJobSize=10000
PriorityWeightPartition=2000
PriorityWeightQOS=1000000
#
#
# PREEMPTION
PreemptType=preempt/none
#PreemptType=preempt/partition_prio
#PreemptMode=suspend,gang
#
#
# LOGGING AND ACCOUNTING
AccountingStorageEnforce=associations,qos
AccountingStorageHost=servhpcmstr0101
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
AccountingStoreJobComment=YES
AccountingStorageTRES=gres/gpu
ClusterName=HPCCluster
#DebugFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
JobContainerType=job_container/none
#JobAcctGatherFrequency=network=60,task=60
JobAcctGatherFrequency=task=30
JobAcctGatherType=jobacct_gather/cgroup
JobAcctGatherParams=NoShared,UsePSS,NoOverMemoryKill
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
SlurmSchedLogFile=/var/log/slurm_scheduler.log
SlurmSchedLogLevel=1
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
GresTypes=gpu
NodeName=nodes[01-1000] RealMemory=120000 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 State=UNKNOWN
NodeName=servgpu        RealMemory=280000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=4 Feature=v100 Gres=gpu:v100:2 State=UNKNOWN
NodeName=serv    RealMemory=250000 CPUs=48 Boards=1 SocketsPerBoard=2 CoresPerSocket=12 ThreadsPerCore=2 State=UNKNOWN
NodeName=servgpu2       RealMemory=250000 Sockets=2 CoresPerSocket=28 ThreadsPerCore=1 Feature=a100 Gres=gpu:a100:2 State=UNKNOWN
#
#
# PARTITIONS
PartitionName=DEFAULT       Nodes=nodes[05-42]     MaxTime=7-00:00:00 Default=NO  State=UP
PartitionName=prio10     Priority=10                  OverSubscribe=NO         TRESBillingWeights="CPU=1.0"              AllowAccounts=ALL            Default=YES
PartitionName=prio50     Priority=50                     OverSubscribe=NO         TRESBillingWeights="CPU=1.0"           AllowAccounts=account1
PartitionName=prio90     Priority=90                   OverSubscribe=EXCLUSIVE  TRESBillingWeights="CPU=1.0"             AllowAccounts=account2
PartitionName=DEFAULT       Nodes=servpwr901             MaxTime=3-00:00:00 Default=NO  State=UP
PartitionName=priogpu10     Priority=10                  OverSubscribe=NO         TRESBillingWeights="CPU=1.0,GRES/gpu=1.0" AllowAccounts=accountgpu1
PartitionName=priogpu50    Priority=50                  OverSubscribe=NO         TRESBillingWeights="CPU=1.0,GRES/gpu=1.0" AllowAccounts=accountgpu2
PartitionName=priogpu90    Priority=90                  OverSubscribe=EXCLUSIVE  TRESBillingWeights="CPU=1.0,GRES/gpu=1.0" AllowAccounts=accountgpu3
PartitionName=DEFAULT       Nodes=serv        MaxTime=INFINITE Default=NO  State=UP
PartitionName=lab        Priority=10                  OverSubscribe=NO         TRESBillingWeights="CPU=1.0"              AllowGroups=GR_slurm_lab
PartitionName=DEFAULT       Nodes=servgpu2          MaxTime=28-00:00:00 Default=NO  State=UP
PartitionName=gpu_a100      Priority=10                  OverSubscribe=NO         TRESBillingWeights="CPU=1.0,GRES/gpu=1.0" AllowGroups=GR_slurm_gpu_a100
